{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5: Image Captioning Tests\n",
    "---\n",
    "\n",
    "This is the Test Notebook that goes with **Homework 5: Image Captioning**! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import *\n",
    "from decoder import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function RNNDecoder.call at 0x169eaf9a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test RNN Decoder\n",
    "# DO NOT CHANGE\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "input_size = 4\n",
    "window_size = 16\n",
    "embed_size = 8\n",
    "vocab_size = 2\n",
    "\n",
    "SOLUTION_SHAPE = (input_size, window_size, vocab_size)\n",
    "\n",
    "hidden_size = 32\n",
    "\n",
    "rnn_decoder = RNNDecoder(vocab_size, hidden_size, window_size)\n",
    "out = rnn_decoder(\n",
    "    tf.random.uniform([input_size, 64], minval=0, maxval=vocab_size),\n",
    "    tf.random.uniform([input_size, window_size], maxval=vocab_size)\n",
    ")\n",
    "epsilon = 1e-5\n",
    "assert out.shape == SOLUTION_SHAPE, \"Incorrect output shape\"\n",
    "assert not (tf.reduce_sum(out) > input_size * window_size - epsilon and tf.reduce_sum(out) < input_size * window_size + epsilon), \"Return logits, not probabilities\"\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Transformer Decoder\n",
    "# DO NOT CHANGE\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "input_size = 4\n",
    "window_size = 16\n",
    "embed_size = 8\n",
    "vocab_size = 2\n",
    "\n",
    "SOLUTION_SHAPE = (input_size, window_size, vocab_size)\n",
    "\n",
    "hidden_size = 32\n",
    "\n",
    "transformer_decoder = TransformerDecoder(vocab_size, hidden_size, window_size)\n",
    "out = transformer_decoder(\n",
    "    tf.random.uniform([input_size, 64], minval=0, maxval=vocab_size),\n",
    "    tf.random.uniform([input_size, window_size], maxval=vocab_size)\n",
    ")\n",
    "epsilon = 1e-5\n",
    "assert out.shape == SOLUTION_SHAPE, \"Incorrect output shape\"\n",
    "assert not (tf.reduce_sum(out) > input_size * window_size - epsilon and tf.reduce_sum(out) < input_size * window_size + epsilon), \"Return logits, not probabilities\"\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "[[[1.1799859  0.9170818  1.1751857  1.440592   0.91815513 1.2267926\n",
      "   1.2073946  1.6518013 ]\n",
      "  [1.1794125  0.91572845 1.1756619  1.4374462  0.9186964  1.2262481\n",
      "   1.2067275  1.6494495 ]\n",
      "  [1.1836915  0.9187926  1.1791174  1.439358   0.9231006  1.2313904\n",
      "   1.2079111  1.6512562 ]\n",
      "  [1.1731007  0.91187423 1.1699021  1.4366666  0.91140133 1.218561\n",
      "   1.2053405  1.6482738 ]]\n",
      "\n",
      " [[1.209747   0.9404253  1.2512807  1.3217736  0.98952305 1.3150989\n",
      "   1.2852045  1.625126  ]\n",
      "  [1.2231039  0.9480182  1.2529895  1.3343003  1.0016358  1.3257245\n",
      "   1.2687719  1.6219754 ]\n",
      "  [1.2115346  0.94174653 1.252518   1.32326    0.99113894 1.3170344\n",
      "   1.2855574  1.6262252 ]\n",
      "  [1.2268441  0.9489866  1.2496402  1.3385301  1.0050474  1.3267511\n",
      "   1.2544852  1.6153218 ]]]\n",
      "Solution:\n",
      "[[[1.1887338  0.92231    1.1832824  1.4413241  0.9284028  1.2374643\n",
      "   1.2092555  1.6531762 ]\n",
      "  [1.1876637  0.920028   1.1839285  1.4362004  0.92912036 1.2364106\n",
      "   1.208142   1.6493324 ]\n",
      "  [1.1947478  0.92511594 1.18963    1.4394116  0.9363934  1.2449214\n",
      "   1.2101097  1.6523575 ]\n",
      "  [1.177351   0.9136951  1.1745526  1.4348177  0.91724336 1.2238563\n",
      "   1.2058568  1.6473322 ]]\n",
      "\n",
      " [[1.171719   0.91546506 1.2353648  1.2881929  0.955095   1.2792217\n",
      "   1.3040287  1.6174344 ]\n",
      "  [1.1937302  0.9304202  1.2462558  1.3073134  0.9750139  1.3008424\n",
      "   1.2973801  1.6244173 ]\n",
      "  [1.1734747  0.9167778  1.2366297  1.2896433  0.9566818  1.281148\n",
      "   1.304501   1.618589  ]\n",
      "  [1.204712   0.9370769  1.2490294  1.3173547  0.9849655  1.3102754\n",
      "   1.2873328  1.6238908 ]]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Incorrect output values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 49\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m out\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m SOLUTION\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect output shape\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(out, SOLUTION, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect output values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Incorrect output values",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSolution:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(SOLUTION)\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect output values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTests passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Incorrect output values"
     ]
    }
   ],
   "source": [
    "# Test AttentionHead call function\n",
    "# DO NOT CHANGE\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "SOLUTION = np.array([\n",
    " [[1.1887338,  0.92231,    1.1832824,  1.4413241,  0.9284028,  1.2374643,\n",
    "   1.2092555,  1.6531762, ],\n",
    "  [1.1876637,  0.920028,   1.1839285,  1.4362004,  0.92912036, 1.2364106,\n",
    "   1.208142,   1.6493324, ],\n",
    "  [1.1947478,  0.92511594, 1.18963,   1.4394116,  0.9363934,  1.2449214,\n",
    "   1.2101097,  1.6523575, ],\n",
    "  [1.177351,   0.9136951,  1.1745526,  1.4348177,  0.91724336, 1.2238563,\n",
    "   1.2058568,  1.6473322, ]],\n",
    "\n",
    " [[1.171719,   0.91546506, 1.2353648,  1.2881929,  0.955095,   1.2792217,\n",
    "   1.3040287,  1.6174344, ],\n",
    "  [1.1937302,  0.9304202,  1.2462558,  1.3073134,  0.9750139,  1.3008424,\n",
    "   1.2973801,  1.6244173, ],\n",
    "  [1.1734747,  0.9167778,  1.2366297,  1.2896433,  0.9566818,  1.281148,\n",
    "   1.304501,   1.618589,  ],\n",
    "  [1.204712,   0.9370769,  1.2490294,  1.3173547,  0.9849655,  1.3102754,\n",
    "   1.2873328,  1.6238908, ]]])\n",
    "\n",
    "def create_deterministic_attention_head(input_size, output_size, is_self_attention):\n",
    "    head = AttentionHead(input_size, output_size, is_self_attention)\n",
    "\n",
    "    head.K = tf.random.uniform([input_size, output_size]) # This is not correct. This is for testing purposes only\n",
    "    head.V = tf.random.uniform([input_size, output_size]) # This is not correct. This is for testing purposes only\n",
    "    head.Q = tf.random.uniform([input_size, output_size]) # This is not correct. This is for testing purposes only\n",
    "\n",
    "    return head\n",
    "\n",
    "input_size = 4\n",
    "batch_size = 2\n",
    "window_size_keys = 3\n",
    "window_size_values = 3\n",
    "window_size_queries = 4\n",
    "\n",
    "head = create_deterministic_attention_head(4, 8, False)\n",
    "\n",
    "out = head.call(\n",
    "    tf.random.uniform([batch_size, window_size_keys, input_size], dtype=np.float32),\n",
    "    tf.random.uniform([batch_size, window_size_values, input_size], dtype=np.float32),\n",
    "    tf.random.uniform([batch_size, window_size_queries, input_size], dtype=np.float32)\n",
    ").numpy()\n",
    "\n",
    "assert out.shape == SOLUTION.shape, \"Incorrect output shape\"\n",
    "try: assert np.allclose(out, SOLUTION, rtol=0.001), \"Incorrect output values\"\n",
    "except AssertionError:\n",
    "    print(\"Output:\")\n",
    "    print(out)\n",
    "    print(\"Solution:\")\n",
    "    print(SOLUTION)\n",
    "    assert False, \"Incorrect output values\"\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 8, 4)\n",
      "(2, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test TransformerBlock\n",
    "# DO NOT CHANGE\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "SOLUTION = np.array([\n",
    " [[0.         ,0.6974105  ,0.98577017 ,0.        ],\n",
    "  [0.         ,0.31697747 ,1.4552947  ,0.        ],\n",
    "  [0.         ,0.         ,1.6302229  ,0.        ],\n",
    "  [0.8908619  ,0.761421   ,0.         ,0.        ],\n",
    "  [0.         ,1.093846   ,0.         ,0.85964626],\n",
    "  [0.         ,0.         ,1.1071919  ,0.84442014],\n",
    "  [0.         ,0.7972666  ,0.         ,1.1266519 ],\n",
    "  [0.         ,0.         ,1.6696594  ,0.        ]],\n",
    "\n",
    " [[0.         ,0.         ,1.6952605  ,0.        ],\n",
    "  [0.         ,1.040991   ,0.9518903  ,0.        ],\n",
    "  [0.         ,0.9950613  ,0.19130549 ,0.47112876],\n",
    "  [0.         ,1.2255263  ,0.72682804 ,0.        ],\n",
    "  [0.59422725 ,0.         ,1.3293806  ,0.        ],\n",
    "  [1.56447    ,0.         ,0.         ,0.01488148],\n",
    "  [0.         ,0.6034657  ,0.         ,1.1571136 ],\n",
    "  [0.         ,0.18686305 ,0.29973274 ,1.1265295 ]]])\n",
    "\n",
    "batch_size = 2\n",
    "input_seq_length = 8\n",
    "embedding_size = 4\n",
    "context_seq_length = 6\n",
    "multiheadedattention = False\n",
    "\n",
    "transformer_block = TransformerBlock(embedding_size, multiheadedattention)\n",
    "transformer_block.self_atten = create_deterministic_attention_head(embedding_size, embedding_size, True)  # This is not correct. This is for testing purposes only\n",
    "transformer_block.self_context_atten = create_deterministic_attention_head(embedding_size, embedding_size, True)  # This is not correct. This is for testing purposes only\n",
    "transformer_block.ff_layer = tf.keras.layers.Dense(1, kernel_initializer=tf.keras.initializers.constant()) #  # This is not correct. This is for testing purposes only\n",
    "\n",
    "out = transformer_block(\n",
    "    tf.random.uniform([batch_size, input_seq_length, embedding_size]),\n",
    "    tf.random.uniform([batch_size, context_seq_length, embedding_size])\n",
    ").numpy()\n",
    "\n",
    "print(SOLUTION.shape)\n",
    "print(out.shape)\n",
    "assert out.shape == SOLUTION.shape, \"Incorrect output shape\"\n",
    "assert not (out < 0).any(), \"Incorrect output activation\"\n",
    "try: assert np.allclose(out, SOLUTION, rtol=0.001), \"Incorrect output values\"\n",
    "except AssertionError:\n",
    "    print(\"Output:\")\n",
    "    print(out)\n",
    "    print(\"Solution:\")\n",
    "    print(SOLUTION)\n",
    "    assert False, \"Incorrect output values\"\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [4,4] vs. [4,8] [Op:AddV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m positional_encoding \u001b[38;5;241m=\u001b[39m PositionalEncoding(vocab_size, embed_size, window_size)\n\u001b[1;32m     23\u001b[0m positional_encoding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m dummy_layer  \u001b[38;5;66;03m# This is not correct. This is for testing purposes only\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mpositional_encoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m out\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m SOLUTION\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect output shape\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(out, SOLUTION, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect output values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/DeepLearning/homework-5p-image-captioning-arianaazarbal/code/transformer.py:203\u001b[0m, in \u001b[0;36mPositionalEncoding.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    202\u001b[0m     out \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(x, tf\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m tf\u001b[38;5;241m.\u001b[39msqrt(tf\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_size, tf\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_encoding\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/csci1470/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [4,4] vs. [4,8] [Op:AddV2] name: "
     ]
    }
   ],
   "source": [
    "# Test Positional Encoding\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "SOLUTION = np.array([\n",
    " [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
    "   1.0000000e+00,  1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
    " [ 8.4147096e-01,  9.9833414e-02,  9.9998331e-03,  9.9999981e-04,\n",
    "   5.4030228e-01,  9.9500418e-01,  9.9994999e-01,  9.9999952e-01],\n",
    " [ 9.0929741e-01,  1.9866933e-01,  1.9998666e-02,  1.9999987e-03,\n",
    "  -4.1614684e-01,  9.8006660e-01,  9.9980003e-01,  9.9999797e-01],\n",
    " [ 1.4112000e-01,  2.9552022e-01,  2.9995501e-02,  2.9999956e-03,\n",
    "  -9.8999250e-01,  9.5533651e-01,  9.9955004e-01,  9.9999553e-01]])\n",
    "\n",
    "input_size = 4\n",
    "window_size = 4\n",
    "embed_size = 8\n",
    "vocab_size = 2\n",
    "\n",
    "dummy_layer = tf.keras.layers.Dense(embed_size, kernel_initializer=tf.keras.initializers.constant())\n",
    "\n",
    "positional_encoding = PositionalEncoding(vocab_size, embed_size, window_size)\n",
    "positional_encoding.embedding = dummy_layer  # This is not correct. This is for testing purposes only\n",
    "\n",
    "out = positional_encoding.call(tf.random.uniform([input_size, window_size])).numpy()\n",
    "\n",
    "assert out.shape == SOLUTION.shape, \"Incorrect output shape\"\n",
    "try: assert np.allclose(out, SOLUTION, rtol=0.001), \"Incorrect output values\"\n",
    "except AssertionError:\n",
    "    print(\"Output:\")\n",
    "    print(out)\n",
    "    print(\"Solution:\")\n",
    "    print(SOLUTION)\n",
    "    assert False, \"Incorrect output values\"\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
